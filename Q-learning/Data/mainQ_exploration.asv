%% main
%tasks, implement value method
% design some delta value
% switch to Q learning


clear all

%-------------------------------------------------------------------------%
%Weed Parameters
MAX_WEED = 5;       %max height agbot can cut weed
Ndim= 30;           %85; %number of rows

%Note still Ndim x Ndim grid to better emulate Javascript envir. rewards
weed_density = zeros(85, Ndim);
weed_height = zeros(85, Ndim); % = REWARD is sum of height in row, declared after
seed_min = 10;
seed_max = 100;
load('seedBank_30.mat') %fixed seedbank
lambda_row = 0.6:.01:.9;
%seed_bank = randi([seed_min, seed_max],85,Ndim); %min 10, max of 15
%seed_bank = ones(85,Ndim)*20; %fixed seedbank of 20

%-------------------------------------------------------------------------%
% Central Planner
PEI_on = 1;                         % how many rows an agbot can see on each side
Nagents = 2; %5;                    % number of agents
N_store = 20;                       % number of total row rewards to store
agent_speed = 1;                    % feet per second
state = ones(1, Nagents);           % start at (1,1)
available_rows = ones(1,Ndim);      % unoccupied rows where agents can be assigned
rows_visible = zeros(1, Ndim);      % which rows we know reward
N_exp        =  zeros(1, Ndim);     % number of times a row has been visited--> resets after every Ndim rows resets
tot_rewards = zeros(1, Ndim);     % sum of rewards harvested from each row
true_N_exp = zeros(1,Ndim);         % actual count of number of times a row is visited. NEVER RESETS
true_tot_rewards = zeros(1,Ndim); % true count of rewards to calculate averages --> calculate delta

reward_history = zeros(N_store, Ndim);  % keeps track of the avg reward each time a row is visited (may want to tune # of avgs kept)
rew_hist_idx   = ones(1, Ndim);         % keeps track of which row to enter in reward history

total_reward = 0;                       % sum of visible row rewards
avg_reward = 0;                         % approximate rew for unexplored rows
agentCell = cell(1, Nagents);           % Initialize Agent Locations
N_exec = 10;                        % Number of executions of the algorithm

%-------------------------------------------------------------------------%
% Time related parameters
days = 60;  %number of days to run 1 simulation (1 for now, should be 60)
T_delay = 2    *   (24*60*60);  %Time (seconds) before deploying robots  (1 day)
T_KILL = 15;                    %(secs) time to kill a weed seconds
T_down_row = 209/agent_speed;   %length of row / velocity
T_weed = 8*T_KILL;              %time to weed individual square

%-------------------------------------------------------------------------%
V = zeros(1, Ndim);

%-------------------------------------------------------------------------%
%Q Learning parameters
epochs = 200;
gamma = .9;        %time discounted rewards
eps_init = 0.2;     %epsilon greedy
eps_decay= 0;       %how much eps should decay...
alp_init = 0.99;     % Alpha - initial learning rate
alp_decay = 0.5;    % how much alp should decay
alpha = alp_init;





Keys = zeros(nchoosek(Ndim,Nagents) + Nagents-1, 1);    % use to find index in Q look up table
idxToState = zeros(size(Keys,1), Nagents);  % use to find state from index

%-------------------------------------------------------------------------%
% Initializing Q learning with Look-up Table

% NOTE: If Nagents is INCREASED FROM 2, add more nested for loops
idx = 1;
Keys(idx) = hash([1,1]);
for i = 1:Ndim
    for j = i+1:Ndim
        idx = idx + 1;
        currState = [i, j];
        idxToState(idx,:) = currState;
        Keys(idx) = hash(currState);
    end
end

data = zeros(2,N_exec); %reward in first row, episode in second
Q = zeros(idx, Ndim); %Q TABLE for each state action pair
Weed_Environment = zeros(85, Ndim, N_exec);
%load('Q_Table_30.mat')
freq = zeros(size(Keys,1), 1); %number of times a S-A pair has been visited
%%
% How Q table is stored
% for state [1, 5] agents are in row 1 and 5
% --> hashValue = hash([1,5])
% --> find(Keys == hashValue) which returns idx of Q_TABLE

tic
for ep = 1:1%number of times to execute algorithm
    
    % RESET Environment
    weed_density = zeros(85, Ndim);
    weed_height = zeros(85, Ndim);  % = REWARD is sum of height in row, declared later
    load('seedBank_30.mat')         % fixed seedbank
    lambda_row = lambda_row(randperm(length(lambda_row))); %randomly permute lambda each trial
    
    % RESET Row Arrays
    rows_visible = zeros(1, Ndim);          % which rows we know reward / explored rows
    available_rows = ones(1,Ndim);          % unoccupied rows
    N_exp        =  zeros(1, Ndim);         % number of times a row has been visited
    tot_rewards = zeros(1, Ndim);         % sum of rewards harvested from each row (all time points)
    num_rows_exp = 0;
    
    % RESET Rewards
    total_reward = 0;                       % sum of visible row rewards
    avg_reward = 0;                         % approximate rew for unexplored rows
    
    % Initialize Agent Locations to home position [0, 0]
    agentCell = cell(1, Nagents);
    state = zeros(1, Nagents);
    %----------------------length of ONE experiment-------------------------
    ep %disp episode
    
    exploration = true; % use exploration policy
    terminate = false;  %terminal state reached
    for t = 0:(60*60*24*days)
        %every hour, update weeds
        if (mod(t, 60*60) == 0)
            [weed_density, seed_bank, weed_height, R] = weed_grow(weed_height, weed_density, seed_bank, 60*60);
            
        end
        
        if (t > T_delay)
            if(exploration)
                if(size(state(state == 0),2) == Nagents)
                    % random initial state
                    state = randsample(30,2)';
                    
                    for a = 1:Nagents
                        t2row =  time2row(0, state(a), agent_speed);
                        t2kill = time2kill(state(a), weed_density, agent_speed);
                        tend = t + t2row + t2kill;                          % calculate when agent will be free
                        agentCell{a} = agents(a,state(a),1,t,tend);         % initialize agent object
                        available_rows(state(a)) = 0;                                       % row is occupied now
                        tot_rewards(state(a)) = tot_rewards(state(a)) +R(state(a));     % keep track of sum of each row
                        N_exp(state(a)) = N_exp(state(a)) + 1;      % number of times a row is explored
                        
                        true_tot_rewards(state(a)) = true_tot_rewards(state(a))+ R(state(a));   %true count of total reward
                        true_N_exp(state(a)) = true_N_exp(state(a)) + 1;                    %true count, never resets
                        
                        %removes weeds from row specified and accompanying reward, R by
                        [weed_density, weed_height, R] = weedUpdate(agentCell{a}.loc , weed_density, weed_height);
                        num_rows_exp = num_rows_exp + 1;
                    end
                end
                
                [available_rows, rows_visible] = updateRowArrays(state, rows_visible, Nagents, R);
                
                new_state = state;
                for a = 1:Nagents
                    %if agent is free
                    if(agentCell{a}.busy == 0)
                        N_exp(state(a)) = N_exp(state(a)) + 1;
                        true_N_exp(new_state(a)) = true_N_exp(new_state(a)) + 1;
                        
                        % this is for targeted exploration
                        unexplored_rows = ~rows_visible;
                        n = length(unexplored_rows);
                        I = zeros(1,n);
                        
                        I(1) =  unexplored_rows(1) + unexplored_rows(2);
                        I(n) =  unexplored_rows(n-1) + unexplored_rows(n);
                        for i = 2:n-1
                            I(i) = unexplored_rows(i-1) + unexplored_rows(i) + unexplored_rows(i+1);
                        end
                        
                        %calc average reward for nonvisible rows to use for
                        %unexplored/non-visible rows
                        n = size(rows_visible(rows_visible == 1), 2);
                        %removes from reward that can't be seen
                        R = R.*rows_visible;
                        avg_reward = sum(tot_rewards)/sum(N_exp);
                        temp = avg_reward.*unexplored_rows;
                        %assigns avg reward to all unexplored rows (~rows_visible)
                        R = R + temp;
                        
                        V = R.*I;
                        [Values, actions] = sort(V, 'descend');
                        
                        for i = 1:size(state,2)
                            %removing occupied rows from consideration
                            r = find(actions == state(i));
                            actions(r) = [];
                            Values(r) = [];
                        end
                        
                        act = find(max(Values) == Values);
                        if(size(act,2) == 1)
                            action= actions(act); %only one max
                        else
                            temp = datasample(act,1);
                            action = actions(temp);
                        end
                        new_state(a) = action;
                        
                        %%
                        %new_state(a) = getMaxValue(R, rows_visible, state);
                        
                        %move free agent to the row and update
                        agentCell = agentUpdate(a, new_state, state, t, weed_density, agentCell, agent_speed);
                        %check to see if any weed is above the max height (5 inches)
                        terminate = terminalState(agentCell{a}.loc, weed_height, MAX_WEED);
                        
                        %update total_rewards and N_exp
                        N_exp(new_state(a)) = N_exp(new_state(a)) + 1;      %resets after all rows have been explored once
                        tot_rewards(new_state(a)) = tot_rewards(new_state(a)) + R(new_state(a));%resets
                        
                        true_N_exp(new_state(a)) = true_N_exp(new_state(a)) + 1; %true count, never resets
                        true_tot_rewards(new_state(a)) = true_tot_rewards(new_state(a)) + R(new_state(a));% true rewards resets
                        reward_history(rew_hist_idx(new_state(a)), new_state(a)) = true_tot_rewards(new_state(a))/ N_exp(new_state(a));
                        
                        %update row idx for reward history
                        if(rew_hist_idx(new_state(a)) < N_store)
                            rew_hist_idx(new_state(a)) = rew_hist_idx(new_state(a))+1;
                        else
                            rew_hist_idx(new_state(a)) = 1;
                        end
                        
                        %if all rows have been visited n times, see if all
                        %rows have low variance
                        delta = 101;
                        if(size(reward_history(reward_history == 0),1) == 0 )
                            true;
                            delta = sum( (zscore(reward_history).^2).^(1/2))/N_store;
                        end
                        if(size(delta(delta < ) == Ndim)
                            true
                            %exploration = false; %switch to Q learning bit
                        end
                        
                        %after Ndim rows have been explored, reset the average reward estimator
                        if(sum(N_exp) >= Ndim)
                            N_exp = zeros(1,Ndim);
                            tot_rewards =zeros(1,Ndim);
                            avg_reward = 0;
                        end
                        
                        %Weeds row and updates Weed environment
                        [weed_density, weed_height, R] = weedUpdate(agentCell{a}.loc, weed_density, weed_height);
                        
                        %UPDATE ROW ARRAYS after assigning agent a
                        [available_rows, rows_visible] = updateRowArrays(new_state, rows_visible, Nagents, R);
                  
                        state = new_state;
                    else
                        %if agent is busy, check to see if it will be free for the next time step
                        if(t == agentCell{a}.tend - 1)
                            agentCell{a}.busy = 0; %free agent up
                        end
                    end
                    if(terminate)
                        break;
                    end
                end
                
                if(terminate)
                    break;
                end
            end
            
            if(terminate == true)
                break;
            end
            
            %% Q LEARNING PORTION
            if(~exploration)
                
                %update row arrays iterating through all agents
                [available_rows, rows_visible] = updateRowArrays(state, rows_visible, Nagents);
                
                new_state = state;
                %epsilon-greedy exploration policy
                for a = 1:Nagents
                    s =  hash2Index( hash(state), Keys ); %current state hash INDEX
                    freq(s,1) = freq(s,1) + 1;
                    if(agentCell{a}.busy == 0) % Agent free to be assigned new row
                        
                        if(rand > eps_init)
                            % GREEDY action
                            new_state(a) = findMaxQ(Q(s,:), state);
                        else
                            % RANDOM action (eps_init probability)
                            new_state(a) = datasample( find(available_rows == 1), 1);
                        end
                        agentCell= agentUpdate(a, new_state, t, weed_density, agentCell, agent_speed);
                        
                        %checks to see if state is terminal
                        terminate = terminalState(agentCell{a}.loc, weed_height, MAX_WEED);
                        
                        total_reward = total_reward + R(agentCell{a}.loc); %adds reward when agent is assigned new row.
                        s_p = hash2Index( hash(new_state), Keys );          %new state hash INDEX
                        if(mod(ep,20) == 0)
                            alpha = alp_init/(floor(ep/20))^alp_decay;
                        end
                        Q(s,new_state(a)) = Q(s,new_state(a)) +alpha*(R(new_state(a)) + gamma*(max(Q(s_p,:)) - Q(s,new_state(a))) ) ;
                        state = new_state;
                        
                        if(terminate == true)
                            data(1,ep) = total_reward;
                            data(2,ep) = t;
                            Weed_Environment(:,:, ep) = weed_height;
                            break;
                        end
                        
                        %Weeds row and updates Weed environment
                        [weed_density, weed_height, R] = weedUpdate(agentCell{a}.loc, weed_density, weed_height);
                        available_rows(new_state(a)) = 0;   %occupied new row (so other agent can't be assigned in same timestep)
                        rows_visible(new_state(a)) = 0;     %unobserved after weeding
                        
                    else
                        %if agent is busy, check to see if it will be free for next time step
                        if(t == agentCell{a}.tend)
                            agentCell{a}.busy = 0; %free agent
                        end
                    end
                    
                end
                
            end
        end
    end
end

toc

%% NESTED FUNCTIONS

function [available_rows, rows_visible] = updateRowArrays(state, rows_visible, Nagents, R)
    %UPDATE ROW ARRAYS
    Ndim = size(rows_visible,2);
    available_rows = ones(1,Ndim);
    for a = 1:Nagents
        left = max(1, state(a) - 1);
        right = min(Ndim, state(a) + 1);
        % check to see if row is not occupied by agbot
        if ( size(find(state == left ), 2) == 0 && R(left) > 0)
            rows_visible(max(1, left) ) = 1;
        end
        if ( size(find(state == right), 2) == 0 && R(right) > 0)
            rows_visible(min(Ndim, right)) = 1;
        end
        available_rows(state(a)) = 0;       %set to 0 so other agents won't go
        rows_visible(state(a))   = 0;
    end
end


%-------------------------------------------------------------------------%

function [weed_density, weed_height, R] = weedUpdate(row, weed_density, weed_height)
    %% updates weed environment and accompanying reward, R after agent has been assigned to a row
    weed_height(:,row) = zeros(85,1);      % assume instant weeding
    weed_density(:,row) = zeros(85,1);
    R = sum(weed_height,1);
end

%-------------------------------------------------------------------------%

function agentCell  = agentUpdate(idx, new_state, state, tstep, weed_density,agentCell, agent_speed)
    %% updates agents in the cell Array agent cell
    agentCell{idx}.loc = new_state(idx);
    tkill = time2kill(new_state(idx), weed_density, agent_speed);
    t2row =  time2row(new_state(idx), state(idx), agent_speed);
    agentCell{idx}.busy = 1; %busy, 0 is FREE
    agentCell{idx}.tstart = tstep;
    agentCell{idx}.tend = tkill + t2row + tstep;
end

%-------------------------------------------------------------------------%

function index = hash2Index(hashVal, Keys)
    %Converts from hash value to Q table SxA index
    index = find(Keys(:,1) == hashVal);
end

%-------------------------------------------------------------------------%

function hashVal = hash(arr)
    %% hashing for unique tabular index
    arr = sort(arr);
    hashVal = 0;
    pow = size(arr,2)-1;
    for i = 1:size(arr,2)
        hashVal = hashVal + arr(i)*31^(pow);
        pow = pow -1;
    end
    
end

%-------------------------------------------------------------------------%

function terminate = terminalState(row, weed_height, MAX_WEED)
    %% determine if there terminal state is reached
    if(size (weed_height(weed_height(:,:)> MAX_WEED), 1) > 0) %if weed greater than 5 inches, terminate
        terminate = true;
    else
        terminate = false;
    end
end


%-------------------------------------------------------------------------%

function time= time2kill(row, weed_density, agent_speed)
    % calculate time to weed one row (traveling + time to kill) once there
    time = 209/agent_speed; % T_down_row;
    for i = 1:size(weed_density,1)
        time = time + min(120, 15*weed_density(i, row));
    end
    
end

%-------------------------------------------------------------------------%
function time = time2row(x1, x2, agent_speed)
    %calculate time to travel to next row
    time = abs(x1 - x2)/agent_speed; %T_to_row
end